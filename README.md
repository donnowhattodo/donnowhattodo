
# Hi there, I'm Kazi Asad ğŸ‘‹
![Typing SVG](https://readme-typing-svg.demolab.com/?lines=Welcome+to+my+GitHub!;NLP+and+ML+enthusiast;Building+solutions+for+tomorrow!&center=true&width=500&height=50)

I'm Asad, an NLP and machine learning enthusiast with a strong foundation in computer science and hands-on research experience. My journey spans across **Natural Language Processing(NLP)**, machine learning, deep learning, and web developmentâ€”each step driven by a passion for creating impactful solutions that harness the power of language and data. Iâ€™m always eager to explore innovative ways to apply these technologies to real-world challenges.

## ğŸ“ Education

**[North South University](https://www.northsouth.edu)**, Dhaka, Bangladesh  

- **M.S. in Computer Science and Engineering**  
  _July 2024 â€“ Present_

- **B.S. in Computer Science and Engineering**  
  _May 2019 â€“ June 2023_  
  **GPA:** 3.53/4.00  
  _Distinction: Cum Laude_

## ğŸš§ Ongoing Projects
![Coding GIF](https://media.giphy.com/media/L1R1tvI9svkIWwpVYr/giphy.gif)

- **Model Distillation in NLP**: Crafting a custom language model, `lastBERT`, distilled from BERT-BASE-UNCASED & BERT-LARGE-UNCASED
- **Community Detection for Gene Prediction**: Comparing graph vs. hypergraph methods in community detection for gene function insights
- **Automated Code Vulnerability Detection**: Using NLP to spot vulnerabilities in open-source code repositories
- **Vision Transformers for Medical Imaging**: Classifying bone fractures accurately with advanced Transformer architectures

## ğŸ“ Undergraduate Research (Under Review)

- **Larger models yield better results? Streamlined severity classification of ADHD-related concerns using BERT-based knowledge distillation**  - [![Model Card](https://img.shields.io/badge/model-LastBERT-blue)](https://huggingface.co/Peraboom/LastBERT)
  *Exploring Knowledge Distillation for BERT-Based Models*: This project aims to create a smaller, efficient language model, `LastBERT`, using BERT-based knowledge distillation techniques. Our model, with only 29M parameters, retains competitive performance while being lightweight and faster. [LastBERT](https://www.medrxiv.org/content/10.1101/2024.10.30.24316411v1)

- **Strengthening Fake News Detection**  
  *Leveraging SVM and Sophisticated Text Vectorization*: This study evaluates the effectiveness of traditional SVM classifiers with advanced vectorization techniques for fake news detection, challenging the necessity of larger transformer models like BERT in certain text classification scenarios.  [FakeNews Detection](https://paperswithcode.com/paper/strengthening-fake-news-detection-leveraging)


## ğŸ¯ Always Learning

- Diving deep into NLP and fine-tuning language models for specialized tasks
- Mastering TensorFlow and PyTorch to push the boundaries of deep learning in NLP

## ğŸŒ Let's Connect

Iâ€™m excited to meet others in the field! Letâ€™s discuss ideas, collaborate on projects, or just share tech insights:

- **LinkedIn**: [kaziasadcse](https://www.linkedin.com/in/kaziasadcse/)
- **Twitter**: [@ImKaziAsad](https://twitter.com/ImKaziAsad)

ğŸ“¬ You can also reach me at [kazi.asad@northsouth.edu](mailto:kazi.asad@northsouth.edu).

## ğŸ¤ Open for Collaboration

Looking to work on impactful NLP, ML, or deep learning projects? Iâ€™m always open to exploring innovative ideas and collaborating on exciting new ventures.

---

## ğŸŒŸ Fun Facts

- **Animeholic** ğŸ¥: Anime inspires me with its unique storytelling and art, sparking creativity in unexpected ways.

![GitHub Streak](https://github-readme-streak-stats.herokuapp.com/?user=donnowhattodo)  
![Visitor Badge](https://visitor-badge.laobi.icu/badge?page_id=donnowhattodo.donnowhattodo)

